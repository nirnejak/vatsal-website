import Image from "next/image"
import getMetadata from "@/utils/metadata"

import CoverImage from "./banner.jpg"
import lora from "./LoRA.png"
import tmi from "./tm_identify.png"
import tmemb from "./tm_embedding.png"
import tmqv from "./tm_qv.png"

export const metadata = getMetadata({
  path: "/blogs/lora-low-rank-adaptation-for-efficient-fine-tuning/",
  title: "LoRA: Low-Rank Adaptation for Efficient Fine-Tuning",
  description:
    "Learn how Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language models by reducing trainable parameters.",
  image: CoverImage.src,
})

# LoRA: Low-Rank Adaptation for Efficient Fine-Tuning

<span className="text-base leading-snug text-neutral-500">
  Learn how Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large
  language models by reducing trainable parameters.
</span>

<Image src={CoverImage} alt={"LoRA Cover"} placeholder="blur" />

## Introduction

Over the past few years, large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks, such as machine translation, question answering, and text generation. These models continue to scale in size (billions and even trillions of parameters), providing increasingly better performance. However, this performance comes with substantial computational and memory requirements, especially when we want to fine-tune a large model for a specific downstream task or domain.

**LoRA (Low-Rank Adaptation)** was introduced as a method to enable parameter-efficient fine-tuning of such large language models. By exploiting low-rank decompositions, LoRA drastically reduces the number of trainable parameters required while still allowing the model to adapt effectively to new tasks.

## Challenges in Fine-Tuning Large Models

### High Cost of Fine-Tuning Large Models

Traditionally, to adapt a pre-trained large language model to a new task (e.g., sentiment classification), one would fine-tune all or most of the parameters of the model on a task-specific dataset. However, this approach has notable drawbacks:

- **Memory**: Updating billions of parameters requires massive GPU/TPU memory.
- **Compute**: Backpropagation through all parameters is computationally expensive.
- **Storage**: Storing a copy of each specialized (fine-tuned) model for different tasks becomes infeasible.
- **Catastrophic Forgetting**: Full fine-tuning may lead to the loss of pre-trained knowledge, making transfer learning less efficient.

### Alternative Parameter-Efficient Methods (and Their Shortcomings)

Several parameter-efficient methods have been proposed to combat these challenges. For example:

- **Feature Extraction**: Freezing the pre-trained model and adding a small trainable head. This is efficient but limits adaptability.
- **Adapter Layers**: Introduce small “adapter” layers between existing layers but keep the main model weights frozen. Effective but increases inference latency.
- **Prefix Tuning / Prompt Tuning**: Add trainable “prefix” tokens or prompts to condition the model on specific tasks.

While these methods reduce the need to retrain all model parameters, they can still exhibit significant overhead in terms of added complexity or limited expressiveness in certain scenarios.

### Low-Rank Adaptation: The LoRA Approach

LoRA offers a simpler yet powerful idea: **any weight update in a large neural network can be approximated by a low-rank matrix factorization.** Instead of learning an entire matrix of updates for a layer’s weights, LoRA learns two smaller matrices whose product approximates the full update. This reduces the total number of trainable parameters and requires much less GPU memory during fine-tuning.

## Intuition and High-Level View

Consider a single linear layer (e.g., a fully connected or dense layer) in a transformer or other deep architectures. The layer’s weight matrix is $W \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}$.

When fine-tuning in the standard way, we would compute some update $\Delta W$ and set the new weights to $W + \Delta W$. However, $\Delta W$ could be as large as $W$ itself. If $d_{\text{out}} \times d_{\text{in}}$ is enormous (as is common in LLMs), updating and storing these large $\Delta W$ matrices for all layers is memory- and compute-intensive.

LoRA’s key insight is that $\Delta W$ **can be assumed to be low-rank**. That is, instead of learning a full matrix $\Delta W$, we learn:

$$
\Delta W = B A
$$

where $A \in \mathbb{R}^{r \times d_{\text{in}}}$ and $B \in \mathbb{R}^{d_{\text{out}} \times r}$. The rank $r$ is typically much smaller than either $d_{\text{out}}$ or $d_{\text{in}}$, so the total number of parameters in $A$ and $B$ is far less than in $\Delta W$.

Hence, during fine-tuning, we only train these low-rank matrices $A$ and $B$ (with rank $r$), while **freezing** the original weights $W$. This drastically **reduces the number of trainable parameters** and, correspondingly, the fine-tuning computational requirements.

## Going Deeper

Let’s break down the various components and steps of LoRA. We’ll focus on a single linear layer for clarity, but this method is applied to multiple layers throughout the network.

### Original Layer and the Low-Rank Decomposition

- **Original weight matrix**: $W \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}$
- **LoRA decomposition**: $\Delta W = B A$

  - $B \in \mathbb{R}^{d_{\text{out}} \times r}$
  - $A \in \mathbb{R}^{r \times d_{\text{in}}}$

  <Image src={lora} alt={"LoRA"} placeholder="blur" />

Here, $r$ is a hyperparameter chosen such that $r \ll d_{\text{in}}$ and $r \ll d_{\text{out}}$. Typically, $r$ might be set to something quite small (e.g., 1, 2, 4, or 8) relative to the dimensions of the layer.

The resulting weight for the layer during fine-tuning becomes:

$$
W' = W + \Delta W = W + B A.
$$

### Forward Pass

During the forward pass of the neural network:

1. **Freeze** $W$: The pre-trained matrix W is not updated; it remains fixed as learned from the original large-scale training.
2. **Add the LoRA adaptation**: We compute the product $B A \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}$. Then, the effective weight is $W + B A$.
3. **Apply to Input**: For an input vector (or batch) $x \in \mathbb{R}^{d_{\text{in}}}$, the layer’s output is:
   $$
   y = (W + B A)x = Wx + B (A x)
   $$
   Because $A x \in \mathbb{R}^{r}$ is a much smaller multiplication (since $A$ compresses the dimension down to $r$), the additional computation cost is minimal.
4. Compute the model’s output $\hat{y}$.

### **Compute loss**:

- $\mathcal{L}(\hat{y}, y)$ (e.g., cross-entropy).

### Backward Pass (Gradient Computation)

During backpropagation:

1. **Gradients w.r.t.** $W$ **are zero** (since W is frozen).
   $$
   \frac{\partial \mathcal{L}}{\partial W} = 0
   $$
2. **Gradients flow through** $B$ **and** $A$: The only parameters that get updated are $B$ and $A$.
   - Compute $\frac{\partial \mathcal{L}}{\partial A}$ and $\frac{\partial \mathcal{L}}{\partial B}$.
   - Update LoRA parameters
     $$
     A \leftarrow A - \eta \frac{\partial \mathcal{L}}{\partial A}, \quad
     B \leftarrow B - \eta \frac{\partial \mathcal{L}}{\partial B}
     $$
     (where $\eta$ is the learning rate)

Therefore, memory usage is significantly reduced. We do not need to store large gradients or optimizers for $W$. Instead, we only store and update the smaller gradients for $A$ and $B$.

## Balancing Adaptation and Stability in LoRA

When we incorporate LoRA into a model, the full update can be scaled using a factor $\alpha$:

$$
\Delta W = \frac{\alpha}{r} B A
$$

### **Impact of Alpha ($\alpha$)**

- **If $\alpha$ is too large**

  LoRA modules may overpower the original pre-trained weights, leading to potential **overfitting** or **instability**.

- **If $\alpha$ is too small**

  The adaptation may be **too weak**, making it difficult to learn new tasks effectively.

- **Common practice**

  Values like **8 or 16** are often chosen for **moderate adaptation**.  
  The optimal $\alpha$ may depend on $$r$$, the model size, and the complexity of the fine-tuning task.

### **Striking the Right Balance: The Effect of** $\frac{1}{r}$

- **Ensuring Rank-Invariant Updates**

  The trainable matrices **A** and **B** have dimensions that scale with $r$. Without normalization, increasing **r** would naturally result in **larger** updates. Dividing by $r$ ensures that the **overall magnitude of the update (**$\Delta W$**) remains consistent**, regardless of the chosen rank.

- **Preventing Training Instability**

  If we scale the update by $\alpha$ without dividing by $r$, increasing $r$ would cause $\Delta W$ to become arbitrarily large, potentially leading to unstable training dynamics and poor convergence.

- **Balancing Adaptation and Stability**

  LoRA aims to introduce **efficient fine-tuning** without disrupting the pre-trained model’s existing knowledge. The $\alpha / r$ scaling ensures that updates are **large enough** to learn new patterns but **not so large** that they overshadow or destabilize the original model.

## Where to Inject LoRA ?

The `target_modules` parameter (specific to Hugging Face’s PEFT library) is used to **specify which modules inside the neural network should be modified by LoRA.**

In transformer-based architectures like GPT, BERT, or T5, certain submodules (like the query, key, value, or dense layers) are responsible for most of the computation and learning capacity. Applying LoRA to **only these parts** allows efficient adaptation while keeping the rest of the model untouched.

<table>
  <thead>
    <tr>
      <th>Model Type</th>
      <th>Common target_modules values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GPT-style</td>
      <td>`["q_proj", "v_proj"]`</td>
    </tr>
    <tr>
      <td>BERT-style</td>
      <td>`["query", "value"]`</td>
    </tr>
    <tr>
      <td>LLaMA</td>
      <td>`["q_proj", "k_proj", "v_proj", "o_proj"]`</td>
    </tr>
  </tbody>
</table>

While LoRA is most commonly applied to **attention** and **feed-forward** layers, it can also be applied to **embedding layers.** This is particularly useful when:

- Introducing **domain-specific terms** (e.g., medical, legal, financial vocabularies).
- Adapting to **multilingual datasets** where token semantics may differ subtly.
- The **pre-trained embeddings are misaligned** with the task-specific distribution.

### Inspecting the Model to Identify Target Modules

A quick way to see which modules could be good candidates for LoRA is to print out the model architecture. Below is a short snippet using **BERT** for sequence classification. You can inspect the module names (e.g., `query`, `value`, etc.) to decide which parts you want to train via LoRA:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Enable gradient checkpointing and disable caching
# (optional but common for LoRA training to save memory)
model.gradient_checkpointing_enable()
model.config.use_cache = False

# Print the full model to inspect its submodules
print(model)
```

This output lets you pinpoint which submodules (e.g., `BertSelfAttention.query`, `BertSelfAttention.value`, or even embeddings like `word_embeddings`) you might include in your `target_modules` list.

<Image src={tmi} alt={"LoRA"} placeholder="blur" />

For a few standard and well-known architectures, target modules are already defined in the [PEFT library implementation of LoRA](https://github.com/huggingface/peft/blob/2ee02af9d445cda0410c43a6e72160df59ad971c/src/peft/utils/constants.py#L85).

### Applying LoRA and Verifying Changes

Once you know the names of the modules you want to adapt, you can apply LoRA using the PEFT library. Below is an example configuration that applies LoRA to **word embedding, query, and value** layers in the BERT model:

```python
from peft import get_peft_model, LoraConfig, TaskType

lora_config = LoraConfig(
    r=8,                          # Low-rank dimension
    lora_alpha=32,                # Scaling factor
    target_modules=["word_embeddings", "query", "value"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_CLS
)

model = get_peft_model(model, lora_config)

# Print the model again to see how LoRA modules are added
print(model)
```

When you print the model after applying `get_peft_model`, you’ll see additional LoRA-related layers attached to the original modules you specified. These added layers represent the **low-rank adapters** that enable you to fine-tune only a small fraction of your network parameters.

<Image src={tmemb} alt={"LoRA"} placeholder="blur" />

<Image src={tmqv} alt={"LoRA"} placeholder="blur" />

**`lora_magnitude_vector`** comes from an optional feature to train a magnitude or scaling vector for LoRA. In many configurations it remains unused (hence an empty `ModuleDict`). It’s simply there to support certain LoRA variants.

When applying LoRA using `get_peft_model`, you need to specify the **task type** so that PEFT knows:

- Where to inject LoRA adapters (e.g., which modules),
- How to prepare the model for that task (e.g., classification vs. generation).

Supported `task_type` values (as of PEFT v0.9+):

<table>
  <thead>
    <tr>
      <th>Task Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>`TaskType.CAUSAL_LM`</td>
      <td>Causal Language Modeling (e.g., GPT-2)</td>
    </tr>
    <tr>
      <td>`TaskType.SEQ_2_SEQ_LM`</td>
      <td>Sequence-to-Sequence Language Modeling (e.g., T5, BART)</td>
    </tr>
    <tr>
      <td>`TaskType.SEQ_CLS`</td>
      <td>Sequence Classification (e.g., BERT, RoBERTa for sentiment)</td>
    </tr>
    <tr>
      <td>`TaskType.TOKEN_CLS`</td>
      <td>Token Classification (e.g., NER tasks)</td>
    </tr>
    <tr>
      <td>`TaskType.MULTIPLE_CHOICE`</td>
      <td>Multiple Choice QA tasks (e.g., SWAG, RACE)</td>
    </tr>
    <tr>
      <td>`TaskType.SPEECH_SEQ_2_SEQ`</td>
      <td>Speech to text models (e.g., Whisper)</td>
    </tr>
    <tr>
      <td>`TaskType.IMAGE_CLASSIFICATION`</td>
      <td>Vision models (e.g., ViT for image classification)</td>
    </tr>
    <tr>
      <td>`TaskType.QUESTION_ANSWERING`</td>
      <td>Extractive QA (e.g., SQuAD with BERT)</td>
    </tr>
    <tr>
      <td>`TaskType.TRANSLATION`</td>
      <td>Text translation tasks</td>
    </tr>
    <tr>
      <td>`TaskType.OTHER`</td>
      <td>For custom or unknown task types — lets you handle it manually</td>
    </tr>
  </tbody>
</table>

## Parameter Saving

LoRA makes it possible to save just the two matrices $A$ and $B$ (and related optimizer states) instead of the entire model. When deployed, you can combine $\Delta W = B A$ with $W$ on-the-fly (or keep them separate, depending on the framework). This results in very compact “adapters” that can be swapped in to adapt a single large pre-trained model to various tasks.

```python
# After fine-tuning, save LoRA parameters only
model.save_pretrained("./lora_adapter")

# Loading LoRA Parameters into the original model later
from transformers import AutoModelForSequenceClassification
from peft import PeftModel

model_name = "bert-base-uncased"
base_model = AutoModelForSequenceClassification.from_pretrained(model_name)
model_with_lora = PeftModel.from_pretrained(base_model, "./lora_adapter")

# The model now has the LoRA adapters loaded and is ready for inference or further fine-tuning.
```

If you need to rapidly switch between multiple tasks, it can be useful to keep LoRA adapters separate and inject them at inference. However, if you only need to serve one specialized model, a one-time merge can drastically streamline your production code.

```python
# Merge LoRA adapters into the base model's weights
merged_model = model_with_lora.merge_and_unload()

# Save the combined model
merged_model.save_pretrained("./merged_model")

# Load the merged model directly later
combined_model = AutoModelForSequenceClassification.from_pretrained("./merged_model")
```

This method provides additional efficiency during deployment by eliminating the adapter overhead completely.

## Advantages and Limitations

### Advantages

1. **Parameter-Efficient**: Only the low-rank matrices $A$ and $B$ need training and storage.
2. **Memory Savings**: Freezing the original weights reduces GPU memory usage.
3. **Modular Adaptations**: You can maintain multiple sets of $A$ and $B$ (one per task or domain) for a single large base model.
4. **Simplicity**: The approach is straightforward to implement on top of existing deep learning frameworks.

### Limitations

1. **Rank Selection**: Choosing an appropriate rank $r$ can be task-specific. If $r$ is too low, the model might underfit; if too high, you lose efficiency benefits.
2. **Assumption of Low-Rank Updates**: In certain highly specialized tasks, the weight update might not be accurately approximated by low-rank factors, leading to suboptimal performance compared to full fine-tuning.
3. **Potential Overhead**: Although smaller than full fine-tuning, LoRA still introduces some overhead. For extremely large models with many layers, the sums can accumulate if not well-managed.

## Conclusion

LoRA (Low-Rank Adaptation) is a powerful technique designed to tackle the challenge of **efficiently adapting large language models** to new tasks. By factoring weight updates into low-rank matrices, LoRA requires **significantly fewer trainable parameters**, reducing the memory footprint and computational overhead associated with full fine-tuning. This makes it a compelling option for scenarios where resources are limited or when multiple domain/task adaptations of a single large model need to be maintained.
