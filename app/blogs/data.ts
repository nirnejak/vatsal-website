export const allBlogs = [
  {
    slug: "rag-evaluation-part-2-generator-evaluation",
    title: "RAG Evaluation Part 2: Generator Evaluation",
  },
  {
    slug: "rag-evaluation-part-1-retriever-evaluation",
    title: "RAG Evaluation Part 1: Retriever Evaluation",
  },
  {
    slug: "evaluation-metrics-for-synthetic-qa-datasets-in-rag-evaluation",
    title: "Evaluation Metrics for Synthetic QA Datasets in RAG Evaluation",
  },
  {
    slug: "rag-enhancing-language-models-with-external-knowledge",
    title: "RAG: Enhancing Language Models with External knowledge",
  },
  {
    slug: "annoy-and-efficient-approximate-nearest-neighbor-algorithm",
    title: "Annoy and Approximate Nearest Neighbor Algorithm",
  },
  {
    slug: "the-react-agent-framework",
    title: "The ReACT Agent Framework",
  },
  {
    slug: "lora-low-rank-adaptation-for-efficient-fine-tuning",
    title: "LoRA: Low-Rank Adaptation for Efficient Fine-Tuning",
  },
  {
    slug: "t5-the-text-to-text-transfer-transformer",
    title: "T5: The Text-to-Text Transfer Transformer",
  },
  {
    slug: "from-gpt-1-to-gpt-3-a-new-era-in-nlp",
    title: "From GPT-1 to GPT-3: A New Era in NLP",
  },
  {
    slug: "a-comprehensive-overview-of-bert",
    title: "A Comprehensive Overview of BERT",
  },
  {
    slug: "understanding-the-transformer-architecture",
    title: "Understanding the Transformer Architecture",
  },
  {
    slug: "self-attention",
    title: "Self-Attention: Queries, Keys, and Values in Action",
  },
  {
    slug: "understanding-scaling-in-self-attention",
    title: "Understanding scaling factor in Self-Attention",
  },
  {
    slug: "normalization-in-deep-learning",
    title: "Normalization in Deep Learning",
  },
  {
    slug: "understanding-padding-and-look-ahead-mask-in-the-transformer-decoder",
    title: "Padding and Look-Ahead Mask in the Transformer Decoder",
  },
  {
    slug: "encoder-decoder-attention-in-the-transformer",
    title: "Encoder - Decoder Attention in the Transformer",
  },
  {
    slug: "sinusoidal-positional-encoding-in-the-transformer",
    title: "Sinusoidal Positional Encoding in the Transformer",
  },
  {
    slug: "attention-mechanism",
    title: "Attention Mechanism in Encoder - Decoder Architecture",
  },
  {
    slug: "encoder-decoder-architecture",
    title: "Seq2Seq Learning - An Encoder-Decoder Approach",
  },
  {
    slug: "model-evaluation-sensitivity-specificity-and-roc-auc",
    title: "Model Evaluation: Sensitivity, Specificity, and ROC-AUC",
  },
  {
    slug: "a-lagrange-multiplier-approach-to-pca",
    title: "A Lagrange Multiplier Approach to PCA",
  },
]
